{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import ast\n",
    "import nltk\n",
    "#nltk.download()\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "df_all=pd.read_csv('wikihowAll.csv')\n",
    "df_all.reset_index(drop=True, inplace=True)\n",
    "#drop rows with null values (0.4% of the datset)\n",
    "df_all=df_all.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data exploration: find out token situation\n",
    "def get_stats(column):\n",
    "    \n",
    "    #simple tokenization and processing\n",
    "    def tok(text):\n",
    "        #lower case\n",
    "        text=text.lower()\n",
    "        #remove punctuation\n",
    "        text = text.translate(str.maketrans('','',string.punctuation))\n",
    "        text = text.translate(str.maketrans('','','1234567890'))\n",
    "        text = re.sub(\"[^a-zA-Z]+\", \" \", text)\n",
    "        #tokenisation\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return tokens\n",
    "    tokens=column.apply(lambda x: tok(x))\n",
    "    len_list=[]\n",
    "    for x in tokens:\n",
    "        length=len(x)\n",
    "        len_list.append(length)\n",
    "    avg=np.mean(len_list)\n",
    "    max_l=np.max(len_list)\n",
    "    min_l=np.min(len_list)\n",
    "    return(\"Average:%d, Max:%d, Min%d\"&(avg,max_l,min_l))\n",
    "#get_stats(df_all['text'])\n",
    "#get_stats(df_all['headline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete preprocessing used for topic modelling\n",
    "def Text_Processing(text):\n",
    "    #lower case\n",
    "    text=text.lower()\n",
    "    #remove punctuation\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    text = text.translate(str.maketrans('','','1234567890'))\n",
    "    text = re.sub(\"[^a-zA-Z]+\", \" \", text)\n",
    "    #tokenisation\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    #stop words\n",
    "    stop_list = set(stopwords.words('english')) \n",
    "    filtered_tokens=[word for word in tokens if word not in stop_list]\n",
    "    #lemmatisation\n",
    "    wnl = WordNetLemmatizer()\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN    \n",
    "    def lemmatize_with_pos(token):\n",
    "        pos = nltk.pos_tag(token)\n",
    "        lemm_words = [wnl.lemmatize(sw[0], get_wordnet_pos(sw[1])) for sw in pos]\n",
    "        return lemm_words\n",
    "    final_token=lemmatize_with_pos(filtered_tokens)\n",
    "    return final_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=df_all['text']\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "#create docuement-word matrix\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df = 0.99,min_df = 0.01,tokenizer = Text_Processing,max_features=no_features)\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "cv_vectorizer = CountVectorizer(max_df = 0.99,min_df = 0.01,tokenizer = Text_Processing, max_features=no_features)\n",
    "cv = tf_vectorizer.fit_transform(corpus)\n",
    "cv_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run topic modelling\n",
    "\n",
    "\n",
    "# Run NMF\n",
    "#nmf_model = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "#lda_model = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find best para for topic modelling\n",
    "\n",
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(cv)\n",
    "\n",
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Log Likelyhoods from Grid Search Output\n",
    "n_topics = [10, 15, 20, 25, 30]\n",
    "log_likelyhoods_5 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.5]\n",
    "log_likelyhoods_7 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.7]\n",
    "log_likelyhoods_9 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.9]\n",
    "\n",
    "# Show graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic%d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic0:\n",
      "like say ask talk conversation know friend something make tell\n",
      "Topic1:\n",
      "cat toy treat breed breeder may play dog food get\n",
      "Topic2:\n",
      "hamster cage food hand treat dwarf may vet wheel bite\n",
      "Topic3:\n",
      "pig guinea cage vet hay vitamin c treat vegetable pellet\n",
      "Topic4:\n",
      "horse bridle halter bit rope hoof mouth tie strap head\n",
      "Topic5:\n",
      "crush person someone feeling people youre friend around dance see\n",
      "Topic6:\n",
      "litter box use cat scoop clean bathroom waste urine may\n",
      "Topic7:\n",
      "relationship friend feel feeling person time ex people thing may\n",
      "Topic8:\n",
      "goat milk hoof kid fence trim feed foot mother baby\n",
      "Topic9:\n",
      "kiss lip eye mouth want move touch close partner first\n",
      "Topic10:\n",
      "cage pet use water make clean bed sure food toy\n",
      "Topic11:\n",
      "coat color tail breed ear eye pattern body round fur\n",
      "Topic12:\n",
      "saddle leather soap stirrup oil pad inch bar dry back\n",
      "Topic13:\n",
      "dont youre guy he doesnt hell thing youll make think\n",
      "Topic14:\n",
      "girl shes like guy look may woman show shell flirt\n",
      "Topic0:\n",
      "time make relationship feel want friend girl people thing new\n",
      "Topic1:\n",
      "kiss lip mouth eye partner want hand touch close tongue\n",
      "Topic2:\n",
      "horse hand hold use make nail bit sure trim try\n",
      "Topic3:\n",
      "saddle use horse coat make hair color sure look tail\n",
      "Topic4:\n",
      "pig guinea cage vet make water need food hay pet\n",
      "Topic5:\n",
      "use make cut need piece sure want place fabric inch\n",
      "Topic6:\n",
      "crush like friend make say feel want dont know ask\n",
      "Topic7:\n",
      "treat train reward training use time target trick clicker hand\n",
      "Topic8:\n",
      "like make look girl dont try guy talk youre want\n",
      "Topic9:\n",
      "hamster cage box litter make use food clean sure water\n",
      "Topic10:\n",
      "goat milk kid need feed mother fence day baby produce\n",
      "Topic11:\n",
      "help need work squirrel cause avoid make use leave state\n",
      "Topic12:\n",
      "stage fleece job gain set need music clean position black\n",
      "Topic13:\n",
      "hamster breed pet fur animal eye baby breeder need kitten\n",
      "Topic14:\n",
      "cat toy pet dog use time like play make room\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 10\n",
    "#display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(best_lda_model, cv_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
